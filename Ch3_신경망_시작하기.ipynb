{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0Rmkw0N72tyh",
        "outputId": "86a634f0-ef58-4c52-c4a5-ecc8685e1045"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 신경망의 구조\n",
        "----\n",
        "신경망 훈련 관련 요소\n",
        "* 네트워크(또는 모델)를 구성하는 층\n",
        "* 입력 데이터와 그에 상응하는 타깃\n",
        "* 학습에 사용할 피드백 신호를 정의하는 손실 함수\n",
        "* 학습 진행 방식을 결정하는 옵티마이저\n"
      ],
      "metadata": {
        "id": "0J9DQ-C33HNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 층: 딥러닝의 구성 단위\n",
        "층은 하나 이상의 텐서를 입력으로 받아 하나 이상의 텐서를 처리하는 데이터 처리 모듈. 대부분의 경우 가중치라는 층의 상태를 가지며, 가중치는 확률 경사 하강법에 의해 학습되는 하나 이상의 텐서로 여기에 네트워크가 학습한 지식이 담겨 있음.\n",
        "\n",
        "층에 따른 적절한 텐서 포맷과 데이터 처리 방식\n",
        "* 벡터 데이터(2D 텐서): 완전 연결 층, 밀집 연결 층(밀집 층)에 의해 처리\n",
        "* 시퀀스 데이터(3D 텐서): LSTM 같은 순환 층에 의해 처리\n",
        "* 이미지 데이터(4D 텐서): 2D 합성곱 픙에 의해 처리\n",
        "\n",
        "층 호환성은 각 층이 특정 크기의 입력 텐서만 받고 특정 크기의 출력 텐서를 반환한다는 사실을 의미."
      ],
      "metadata": {
        "id": "TAUpcr5u8HH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "layer = layers.Dense(32, input_shape=(784,))      # 32개의 유닛으로 된 밀집 층\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, input_shape=(784,)))\n",
        "model.add(layers.Dense(10))"
      ],
      "metadata": {
        "id": "TAAZ4FXP5IYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째 차원이 784인 2D 텐서만 입력으로 받는 층으로 만들었으며(배치 차원인 0번째 축은 지정하지 않기 때문에 어떤 배치 크기도 입력으로 받을 수 있음), 이 층은 첫 번째 차원 크기가 32로 변환된 텐서를 출력. 따라서 32차원의 벡터를 입력으로 받는 하위 층이 연결되어야 함. 이때 케라스에서는 모델에 추가된 층을 자동으로 상위 층의 크기에 맞추어 줌.\n",
        "\n",
        "두 번째 층에는 input_shape를 지정하지 않았으므로 자동으로 앞선 층의 출력 크기를 입력 크기로 설정됨."
      ],
      "metadata": {
        "id": "_mSrk-fx6E19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델: 층의 네트워크\n",
        "딥러닝 모델은 층으로 만든 비순한 유향 그래프로 가장 일반적인 예가 하나의 입력을 하나의 출력으로 매핑하는 층을 순서대로 쌓는 것.\n",
        "\n",
        "자주 등장하는 네트워크 구조\n",
        "* 가지가 2개인 네트워크\n",
        "* 출력이 여러 개인 네트워크\n",
        "* 인셉션 블록\n",
        "\n",
        "네트워크 구조는 가설 공간을 정의하며 네트워크 구조를 선택함으로써 가능성 있는 공간(가설 공간)을 입력에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한하게 됨.\n",
        "\n",
        "*우리가 찾아야 할 것은 이러한 텐서 연산에 포함된 가중치 텐서의 좋은 값.\n"
      ],
      "metadata": {
        "id": "Op_PKw-B7WY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠\n",
        "* 손실 함수(목적 함수): 훈련하는 동안 최소화될 값으로 주어진 문제에 대한 성공 지표가 됨.\n",
        "* 옵티마이저: 손실 함수를 기반으로 네트워크ㅏ 어떻게 업데이트될지 결정. 특정 종류의 확률적 경사 하강법을 구현.\n",
        "\n",
        "문제에 맞지 않는 목적 함수를 선택하는 경우 원하지 않는 일을 수행하는 모델이 만들어질 것. "
      ],
      "metadata": {
        "id": "abi4K0k18CpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 케라스 소개\n",
        "----\n",
        "케라스는 거의 모든 종류의 딥러닝 모델을 간편하게 만들고 훈련시킬 수 있는 파이썬을 위한 딥러닝 프레임워크.\n",
        "\n",
        "케라스 특징\n",
        "* 동일한 코드로 CPU와 GPU에서 실행할 수 있음.\n",
        "* 사용하기 쉬운 API를 가져 딥러닝 모델의 프로토타입을 빠르게 만들 수 있음.\n",
        "* 합성곱 신경망, 순환 신경망을 지원하며 이 둘을 자유롭게 조합하여 사용할 수 있음.\n",
        "* 다중 입력이나 다중 출력 모델, 층의 공유, 모델 공유 등 어떤 네트워크 구조도 만들 수 있음. 즉 적대적 생성 신경망부터 뉴럴 튜링 머신까지 케라스는 기본적으로 어떤 딥러닝 모델에도 적합함."
      ],
      "metadata": {
        "id": "3f_1UBs9-qGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 케라스를 사용한 개발: 빠르게 둘러보기\n",
        "전형적인 케라스 작업 흐름\n",
        "1. 입력 텐서와 타깃 텐서로 이루어진 훈련 데이터를 정의.\n",
        "2. 입력과 타깃을 매핑하는 층으로 이루어진 네트워크(또는 모델)을 정의.\n",
        "3. 손실 함수, 옵티마이저, 모니터링하기 위한 측정 지표를 선택하여 학습 과정을 설정.\n",
        "4. 훈련 데이터에 대해 모델의 fit() 메서드를 반복적으로 호출,\n",
        "\n",
        "모델을 정의하는 방법\n",
        "* 방법1. 가장 자주 사용하는 구조인 층을 순서대로 쌓아 올린 네트워크인 Sequential 클래스\n",
        "* 방법2. 완전히 임의의 구조를 만들 수 있는 비순환 유향 그래프를 만드는 함수형 API\n"
      ],
      "metadata": {
        "id": "BrGIVJCEACaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 방법1. Sequential 클래스 사용(첫 번째 층에 입력 데이터의 크기가 전달)\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# 방법2. 함수형 API 사용(모델이 처리할 데이터 텐서를 만들고 마치 함수처럼 이 텐서에 층을 적용)\n",
        "input_tensor = layers.Input(shape=(784,))\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = models.Model(inputs=input_tensor, outputs=output_tensor)"
      ],
      "metadata": {
        "id": "s2FYCe1-5Ic6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "컴파일 단계에서 학습 과정이 설정되며, 여기에서 모델이 사용할 옵티마이저와 손실 함수, 훈련하는 동안 모니터링하기 위해 필요한 측정 지표를 지정."
      ],
      "metadata": {
        "id": "OsPzbLztCZRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수를 사용하는 가장 흔한 예시\n",
        "from keras import optimizers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='mse', metrics=['accuracy'])\n",
        "\n",
        "#model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3ORNwKDChO_",
        "outputId": "af483c10-d318-4760-a56e-9b9882169638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQKQe97x2tyv"
      },
      "source": [
        "# 영화 리뷰 분류: 이진 분류 예제\n",
        "----\n",
        "\n",
        "2종 분류 또는 이진 분류는 아마도 가장 널리 적용된 머신 러닝 문제일 것. 이 예제에서 리뷰 텍스트를 기반으로 영화 리뷰를 긍정과 부정로 분류하는 법을 배울 것."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59p36MI2ty1"
      },
      "source": [
        "### IMDB 데이터셋\n",
        "\n",
        "인터넷 영화 데이터베이스로부터 가져온 양극단의 리뷰 50,000개로 이루어짐. 훈련 데이터 25,000개와 테스트 데이터 25,000개로 나뉘어 있고 각각 50%는 부정, 50%는 긍정 리뷰로 구성되어 있음.\n",
        "\n",
        "MNIST 데이터셋처럼 IMDB 데이터셋도 케라스에 포함되어 있으며, 전처리되어 있어 각 리뷰(단어 시퀀스)가 숫자 시퀀스로 변환되어 있음. \n",
        "\n",
        "*각 숫자는 사전에 있는 고유한 단어를 나타냄."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUWclKFf2ty5",
        "outputId": "96794fbe-0fc5-42cf-bd3a-a661a7708d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 로드\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# num_words=10000: 훈련 데이터에서 가장 자주 나타나는 단어 1만 개만 사용하겠다는 의미(드물게 나타나는 단어는 무시)\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX_419R12ty8"
      },
      "source": [
        "변수 `train_data`와 `test_data`는 리뷰의 목록. 각 리뷰는 단어 인덱스의 리스트(단어 시퀀스가 인코딩된 것). `train_labels`와 `test_labels`는 부정을 나타내는 0과 긍정을 나타내는 1의 리스트."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk35OSgp2ty9",
        "outputId": "0c76e9c9-7287-48e4-fc7c-ce8768782b40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q52HxB12ty_",
        "outputId": "8cca532c-9f47-4c7f-a977-2cf9aa95f9e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsHF8YDy2tzD",
        "outputId": "53a41b27-0f1f-49ee-f3a1-2c2e82316320"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# index의 max\n",
        "max([max(sequence) for sequence in train_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ5eUqxC2tzH",
        "outputId": "65b33239-1cc3-494d-dbee-c5d476eb818c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "## 리뷰 데이터 하나를 원래 영어 단어로 어떻게 바꾸는지\n",
        "\n",
        "# word_index는 단어와 정수 인덱스를 매핑한 딕셔너리\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# 정수 인덱스와 단어를 매핑하도록 뒤집음\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# 리뷰를 디코딩\n",
        "# 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위한 인덱스이므로 3을 뺌\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "D_ZgH6Px2tzJ",
        "outputId": "9a45d220-2b64-419c-ebf7-6fa2329f8ce2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "decoded_review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LilQ5jut2tzM"
      },
      "source": [
        "### 데이터 준비\n",
        "\n",
        "신경망에 숫자 리스트를 주입할 수는 없음.\n",
        "\n",
        "리스트를 텐서로 바꾸는 두 가지 방법\n",
        "\n",
        "* 같은 길이가 되도록 리스트에 패딩을 추가하고 `(samples, sequence_length)` 크기의 정수 텐서로 변환하고 이 정수 텐서를 다룰 수 있는 층을 신경망의 첫 번째 층으로 사용함(`Embedding` 층을 말함).\n",
        "* 리스트를 원-핫 인코딩하여 0과 1의 벡터로 변환하고 부동 소수 벡터 데이터를 다룰 수 있는 `Dense` 층을 신경망의 첫 번째 층으로 사용함.\n",
        "\n",
        "여기서는 두 번째 방식을 사용"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Ch3_신경망 시작하기",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}